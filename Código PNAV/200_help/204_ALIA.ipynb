{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div style=\"display: flex; align-items: center; justify-content: space-between;\">\n",
    "  <img src=\"../../.images/PNAV-logo.png\" alt=\"Logo del PNAV\" style=\"width: auto; max-height: 100px;\">\n",
    "  <img src=\"../../.images/ALIA.webp\" alt=\"Logo de ALIA\" style=\"width: auto; max-height: 100px;\">\n",
    "  <img src=\"../../.images/MITECO-logo_background.png\" alt=\"Logo del MITECO\" style=\"width: auto; max-height: 100px;\">\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "# Modelos de la Familia ALIA\n",
    "\n",
    "## 1. Modelos de texto\n",
    "\n",
    "La familia ALIA ofrece una serie de LLMs de texto multilingües, diseñados para diversas aplicaciones en procesamiento de lenguaje natural.\n",
    "\n",
    "### 1.1. ALIA-40B \n",
    "\n",
    "- **Descripción/Función**:LLM de propósito general.\n",
    "- **Parámetros**: 40 mil millones.\n",
    "- **Idiomas**: 35 lenguas de Europa.\n",
    "- **Entrenamiento**: Desde cero con 7,8 billones de tokens.\n",
    "- **Model Card**: [ALIA-40B en Hugging Face](https://huggingface.co/BSC-LT/ALIA-40b)\n",
    "- <span style=\"color: red;\">No recomendado para el desarrollo en el hackathon por su tamaño.</span>\n",
    "\n",
    "### 1.2. Salamandra-7B\n",
    "\n",
    "- **Descripción/Función**:LLM de propósito general.\n",
    "- **Parámetros**: 7 mil millones.\n",
    "- **Idiomas**: 35 lenguas de Europa.\n",
    "- **Entrenamiento**: Desde cero con 12,9 billones de tokens.\n",
    "- **Model Card**: [Salamandra-7B en Hugging Face](https://huggingface.co/BSC-LT/salamandra-7b)\n",
    "- <span style=\"color: green;\">Recomendado.</span>\n",
    "\n",
    "### 1.3. Salamandra-7B-Instruct\n",
    "\n",
    "- **Descripción/Función**:LLM de propósito general, instruido.\n",
    "- **Parámetros**: 7 mil millones.\n",
    "- **Idiomas**: 35 lenguas de Europa.\n",
    "- **Entrenamiento**: Basado en Salamandra-7B, entrenado con 276 mil instrucciones en inglés, castellano y catalán, recogidas de varios corpus abiertos.\n",
    "- **Model Card**: [Salamandra-7B-Instruct en Hugging Face](https://huggingface.co/BSC-LT/salamandra-7b-instruct)\n",
    "- <span style=\"color: green;\">Recomendado.</span>\n",
    "  \n",
    "### 1.4. Salamandra-2B\n",
    "\n",
    "- **Descripción/Función**:LLM de propósito general.\n",
    "- **Parámetros**: 2 mil millones.\n",
    "- **Idiomas**: 35 lenguas de Europa.\n",
    "- **Entrenamiento**: Desde cero con 12,9 billones de tokens.\n",
    "- **Model Card**: [Salamandra-2B en Hugging Face](https://huggingface.co/BSC-LT/salamandra-2b)\n",
    "- <span style=\"color: green;\">Recomendado.</span>\n",
    "\n",
    "### 1.5. Salamandra-2B-Instruct\n",
    "\n",
    "- **Descripción/Función**:LLM de propósito general, instruido.\n",
    "- **Parámetros**: 2 mil millones.\n",
    "- **Idiomas**: 35 lenguas de Europa.\n",
    "- **Entrenamiento**: Basado en Salamandra-2B, entrenado con 276 mil instrucciones en inglés, castellano y catalán, recogidas de varios corpus abiertos.\n",
    "- **Model Card**: [Salamandra-2B-Instruct en Hugging Face](https://huggingface.co/BSC-LT/salamandra-2b-instruct)\n",
    "- <span style=\"color: green;\">Recomendado.</span>\n",
    "\n",
    "## 2. Modelos de traducción\n",
    "\n",
    "La familia ALIA ofrece una serie de modelos de traducción automática multilingüe, diseñados para facilitar la conversión de texto entre diversos idiomas.\n",
    "\n",
    "### 2.1. salamandraTA-2b\n",
    "\n",
    "- **Descripción/Función**: Modelo de traducción automática multilingüe entrenado con 70.000 millones de tokens de datos paralelos en 30 idiomas diferentes. Es el primer modelo de la serie SalamandraTA, entrenado para traducción a nivel de frases y párrafos.\n",
    "- **Parámetros**: 2 mil millones.\n",
    "- **Model Card**: [salamandraTA-2b en Hugging Face](https://huggingface.co/BSC-LT/salamandraTA-2B)\n",
    "\n",
    "### 2.2. Plume256k\n",
    "\n",
    "- **Descripción/Función**: Primer modelo de lenguaje masivo (LLM) entrenado desde cero para traducción automática neuronal con datos paralelos centrados en catalán. Posee la misma arquitectura que Gemma-2B.\n",
    "- **Parámetros**: 256 mil.\n",
    "- **Model Card**: [Plume256k en Hugging Face](https://huggingface.co/projecte-aina/Plume256k)\n",
    "\n",
    "### 2.3. Plume128k\n",
    "\n",
    "- **Descripción/Función**: Modelo de lenguaje masivo (LLM) entrenado desde cero para traducción automática neuronal con datos paralelos centrados en catalán. Posee la misma arquitectura que Gemma-2B.\n",
    "- **Parámetros**: 128 mil.\n",
    "- **Model Card**: [Plume128k en Hugging Face](https://huggingface.co/projecte-aina/Plume128k)\n",
    "\n",
    "### 2.4. Plume32k\n",
    "\n",
    "- **Descripción/Función**: Modelo de lenguaje masivo (LLM) entrenado desde cero para traducción automática neuronal con datos paralelos centrados en catalán. Posee la misma arquitectura que Gemma-2B.\n",
    "- **Parámetros**: 32 mil.\n",
    "- **Model Card**: [Plume32k en Hugging Face](https://huggingface.co/projecte-aina/Plume32k)\n",
    "\n",
    "### 2.5. aina-translator-gl-ca\n",
    "\n",
    "- **Descripción/Función**: Modelo de traducción gallego-catalán entrenado desde cero con Fairseq y un conjunto de aproximadamente 75 millones de pares de frases, que incluye datos paralelos auténticos recogidos de la web y datos paralelos sintéticos creados con el traductor Proxecto Nós.\n",
    "- **Model Card**: [aina-translator-gl-ca en Hugging Face](https://huggingface.co/projecte-aina/aina-translator-gl-ca)\n",
    "\n",
    "### 2.6. aina-translator-eu-ca\n",
    "\n",
    "- **Descripción/Función**: Modelo de traducción vasco-catalán entrenado desde cero con Fairseq y un conjunto de aproximadamente 75 millones de pares de frases, que incluye datos paralelos auténticos recogidos de la web y datos paralelos sintéticos creados con el traductor ES-EU HiTZ.\n",
    "- **Model Card**: [aina-translator-eu-ca en Hugging Face](https://huggingface.co/projecte-aina/aina-translator-eu-ca)\n",
    "\n",
    "### 2.7. aina-translator-es-ast\n",
    "\n",
    "- **Descripción/Función**: Modelo de traducción español-asturiano resultado de un ajuste completo del modelo NLLB-200-600M con un corpus español-asturiano.\n",
    "- **Model Card**: [aina-translator-es-ast en Hugging Face](https://huggingface.co/projecte-aina/aina-translator-es-ast)\n",
    "\n",
    "### 2.8. aina-translator-es-an\n",
    "\n",
    "- **Descripción/Función**: Modelo de traducción español-aragonés resultado de un ajuste completo del modelo NLLB-200-600M con un corpus español-aragonés.\n",
    "- **Model Card**: [aina-translator-es-an en Hugging Face](https://huggingface.co/projecte-aina/aina-translator-es-an)\n",
    "\n",
    "### 2.9. aina-translator-es-oc\n",
    "\n",
    "- **Descripción/Función**: Modelo de traducción español-aranés resultado de un ajuste completo del modelo NLLB-200-600M con un corpus español-aranés.\n",
    "- **Model Card**: [aina-translator-es-oc en Hugging Face](https://huggingface.co/projecte-aina/aina-translator-es-oc)\n",
    "\n",
    "Para más información y acceso a los modelos, puedes visitar sus respectivas páginas en Hugging Face y la [web del ALIA Kit](https://langtech-bsc.gitbook.io/alia-kit)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
