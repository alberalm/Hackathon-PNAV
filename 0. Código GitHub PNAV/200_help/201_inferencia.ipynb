{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<div style=\"display: flex; align-items: center; justify-content: space-between;\">\n",
        "  <img src=\"../../.images/PNAV-logo.png\" alt=\"Logo del PNAV\" style=\"width: auto; max-height: 100px;\">\n",
        "  <img src=\"../../.images/MITECO-logo_background.png\" alt=\"Logo del MITECO\" style=\"width: auto; max-height: 100px;\">\n",
        "</div>\n",
        "\n",
        "---\n",
        "\n",
        "# Inferencia con LLMs utilizando Transformers\n",
        "A continuación, se mostrará el proceso de cómo hacer inferencia con el LLM `Qwen2.5-1.5B-Instruct` utilizando la librería `transformers` desde un notebook en Azure.\n",
        "\n",
        "⚠️ Este material es un notebook de demostración **orientativo** sobre la metodología que se debe seguir, no se debe replicar el desarrollo del caso de uso."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 1. Instalación de librerías y paquetes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The system cannot find the path specified.\n",
            "The system cannot find the path specified.\n",
            "The system cannot find the path specified.\n"
          ]
        }
      ],
      "source": [
        "!/anaconda/envs/azureml_py310_sdkv2/bin/pip install transformers\n",
        "!/anaconda/envs/azureml_py310_sdkv2/bin/pip install torch\n",
        "!/anaconda/envs/azureml_py310_sdkv2/bin/pip install 'accelerate>=0.26.0'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 2. Importación de módulos\n",
        "Se importan las clases `AutoModelForCausalLM` y `AutoTokenizer` del paquete `transformers`. Estas clases permiten cargar modelos y tokenizadores preentrenados para tareas de modelado de lenguaje causal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 3. Nombre del modelo\n",
        "Se define el nombre del modelo preentrenado que se desea utilizar, en este caso `\"Qwen/Qwen2.5-1.5B-Instruct\"`, un modelo de lenguaje desarrollado por Alibaba Cloud."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 4. Carga del modelo\n",
        "- `AutoModelForCausalLM`:from_pretrained: Carga el modelo preentrenado desde el nombre especificado.\n",
        "- `torch_dtype=\"auto\"`: Configura automáticamente el tipo de dato (por ejemplo, `float32` o `float16`) en función del dispositivo disponible.\n",
        "- `device_map=\"auto\"`: Distribuye el modelo automáticamente en los dispositivos disponibles, como CPU o GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 5. Carga del tokenizador\n",
        "Carga el tokenizador correspondiente al modelo preentrenado. El tokenizador convierte el texto en tensores numéricos que pueden ser procesados por el modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 6. Definición del prompt\n",
        "Se especifica el texto inicial que se proporcionará al modelo como entrada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "prompt = \"Give me a short introduction to large language model.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 7. Definición del contexto\n",
        "Se crea una lista de mensajes, con un mensaje inicial de sistema que define el rol del modelo y un mensaje de usuario que contiene el `prompt`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 8. Preparación del texto\n",
        "- `apply_chat_template`: Convierte la lista de mensajes en un formato de texto comprensible para el modelo.\n",
        "- `tokenize=False`: Indica que el texto generado no debe tokenizarse todavía.\n",
        "- `add_generation_prompt=True`: Añade automáticamente información necesaria para la generación de texto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 9. Tokenización y preparación para el modelo\n",
        "- `tokenizer([text], return_tensors=\"pt\")`: Convierte el texto en tensores de PyTorch (pt).\n",
        "- `.to(model.device)`: Mueve los tensores al mismo dispositivo donde está el modelo (por ejemplo, GPU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 10. Generación de texto\n",
        "- `model.generate`: El modelo genera texto a partir de los tensores de entrada.\n",
        "- `max_new_tokens=512`: Especifica el número máximo de tokens que el modelo puede generar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "generated_ids = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=512\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 11. Extracción de tokens generados\n",
        "Filtra los tokens generados eliminando los tokens de entrada para quedarse solo con los nuevos tokens producidos por el modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "generated_ids = [\n",
        "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 12. Decodificación del texto generado\n",
        "- `tokenizer.batch_decode`: Convierte los tokens numéricos de vuelta a texto legible.\n",
        "- `skip_special_tokens=True`: Elimina tokens especiales del texto final.\n",
        "- `[0]`: Selecciona el primer elemento (ya que normalmente se genera solo una respuesta)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 13. Impresión de la respuesta\n",
        "Muestra el texto generado por el modelo como salida final."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1734685019398
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/cuda/__init__.py:716: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A large language model (LLM) is an artificial intelligence system that has the ability to generate human-like text based on vast amounts of data it has been trained on. These models use deep learning techniques and neural networks to understand and process natural language, allowing them to perform tasks such as translation, summarization, question-answering, and creative writing.\n",
            "\n",
            "Some key features of LLMs include:\n",
            "\n",
            "1. **Training Data**: They are typically trained using massive datasets, often in the billions or trillions of words, which allows them to learn complex patterns and relationships within natural language.\n",
            "\n",
            "2. **Generative Abilities**: LLMs can produce novel text that resembles real human language, making them useful for applications like chatbots, content generation, and even poetry creation.\n",
            "\n",
            "3. **Scalability**: Due to their extensive training, these models can handle very large inputs and outputs efficiently, making them suitable for processing long documents and generating detailed responses.\n",
            "\n",
            "4. **Ethical Considerations**: The deployment of LLMs raises concerns about privacy, bias, and potential misuse. Ensuring fairness and transparency in how they interact with users is crucial.\n",
            "\n",
            "5. **Challenges**: Training LLMs requires significant computational resources and expertise, and there are ongoing debates about the ethical implications of deploying AI systems that can create convincing fake news or other forms of misinformation.\n",
            "\n",
            "Despite these challenges, LLMs have become increasingly sophisticated and widely used across various industries, from customer service to research and beyond.\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 14. Disclaimer\n",
        "⚠️ La forma de hacer inferencia de cada modelo puede variar. Para consultar cómo inferir con un determinado modelo, consultar la tarjeta del modelo en [Hugging Face](https://huggingface.co/)."
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
